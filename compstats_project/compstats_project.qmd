---
title: "Hamiltonian Monte Carlo and the No-U-Turn-Sampler"
subtitle: "Computational Statistics Project"
author: 
- "Jonathan Fitter ([jfitter@wu.ac.at](mailto:jonathan.fitter@wu.ac.at))"
- "Max Heinze ([mheinze@wu.ac.at](mailto:mheinze@wu.ac.at))"
institute:
- "Department of Economics, Vienna University of Economics and Business (WU Vienna)"
date: "2025-05-27"
date-format: long
lang: en
format: 
  revealjs:
    theme: [default, ../style/mhslides_flippedcols.css]
    width: 1280
    height: 720
    margin: 0
    progress: false
    overview: false
    highlight-style: github
    slideNumber: true
    html-math-method: mathjax
    embed-resources: true
    pdfMaxPagesPerSlide: 1
    pdfSeparateFragments: false
    template-partials:
      - ../style/title-slide.html
    filters:
      - ../style/section-header.lua
      - _filters/pdf-to-svg.lua
      - _filters/space.lua
      - quarto-kroki
      - _filters/svgfonts.lua
engine: knitr
bibliography: references.bib
csl: ../style/apa.csl
nocite: |
  @hoffman2014
  @feng2017
  @neal2012
---

# Intro and Recap

## Markov Chain

. . . 

A [**Markov chain**]{.col1} is a stochastic process $\{X_t\}$ indexed by time $t\geq 0$.

:::{.incremental}
* We call the **set of all values** that $\{X_t\}$ can assume the [**state space**]{.col1} of the Markov Chain.
* $\{X-t\}$ is called a Markov Chain if it fulfills the [**Markov property**]{.col1} that
  $$
  P(X_{t+1}=j\mid X_0=i_0,\dots,X_t=i_t)=P(X_{t+1}=j\mid X_t=i_t),
  $$
  i.e., that the conditional distribution of the next state depends only on the current state.
:::

. . .

[**Markov Chain Monte Carlo (MCMC)**]{.col1} methods make use of Markov chains to sample from a **target distribution**. 


## Markov Chain

::::{.columns}
:::{.column width="40%" .fragment}
![](figures/markov.svg){.nostretch width="100%"}
:::
:::{.column width="60%" .fragment}
Markov chains are

:::{.incremental}
* **irreducible**, i.e. every state can eventually be reached from any state;
* **aperiodic**, i.e. reverting to a state is not only possible after a multiple of a certain number of steps;
* **positive recurrent**, i.e. for all states, the expected number of transitions to return to that state is finite.
:::

[For such Markov Chains, [**transition probabilities**]{.col1} converge to a unique stationary distribution on the state space.]{.fragment}

:::
::::

## Metropolis-Hastings

::::{.columns}
:::{.column width="40%" .fragment}

:vspace4


![](figures/metropolis.svg){width="100%"}
:::
:::{.column width="60%" .fragment}

**Metropolis-Hastings** is a class of MCMC methods. Using a Metropolis-Hastings algorithm, we follow these steps:

:::{.incremental}
* We draw the initial value from some density.
*	We selet a proposal density $g(\cdot\mid i)$
* We compute $\alpha(i,j)=\min\Bigl(1,\frac{f(j)\,g(i\mid j)}{f(i)\,g(j\mid i)}\Bigr)$.
* We draw $U$ from $U(0,1)$.
* If $U\le\alpha(i,j)$, we move from $i$ to $j$, otherwise we stay at $i$.
:::



:::
::::




# Hamiltonian Monte Carlo

## Origins in Physics

::::{.columns}
:::{.column width="45%" .fragment .centering}

:vspace3

![](figures/puck.svg){width=100%}
:::
:::{.column width="55%" .fragment}

Imagine a **frictionless puck** on a **non-even surface**. The [**state of this system**]{.col1} is given by

:::{.incremental}
*	$\boldsymbol{q}$, the [**position**]{.col1} of the puck;
* $\boldsymbol{p}$, the [**momentum**]{.col4} of the puck, given by $\mathrm{mass}\times\mathrm{velocity}$.
* If the puck encounters a **rising slope**, momentum will decrease; if it encounters a **falling slope**, momentum will increase.
* The higher the surface at a position, the higher the puck's **potential energy**, and the lower its **kinetic energy**.
:::


:::
::::

<!---

## Origins in Physics

:::{.incremental}
* We simulate the path of the puck starting from a position in this Hamiltonian system with a given kinetic energy
* Pre-determined step-size and number of steps (use of discretizer)
* After stopping at the final position along the path, we randomly introduce a new level of kinetic energy
* we simulate the path,...
:::

-->


## From Physics to Statistics

. . .

:vspace1

In **non-physical** applications, 

:vspace1.5

:::{.incremental .spacey}
* the [**position**]{.col1} $\boldsymbol{q}$ corresponds to the [**variables of interest**]{.col1},
* the [**potential energy**]{.col2} $\boldsymbol{p}$ corresponds to the [**negative log of the probability density**]{.col2} for these variables, and
* the [**momentum**]{.col4} is an auxiliary variable that facilitates exploration of the target distribution space.
:::


## Hamiltonian Dynamics

. . .

The system is described by a so-called [**Hamiltonian equation**]{.col1}, $H(q_p)$. Its partial derivatives, the **equations of motion**, determine how $\boldsymbol{q}$ and $\boldsymbol{p}$ change over time:

. . .

:vspace1.5

$$
\begin{aligned}
\frac{\mathrm{d}q_i}{\mathrm{d}t}&=\frac{\partial H}{\partial p_i},\\
\frac{\mathrm{d}p_i}{\mathrm{d}t}&=-\frac{\partial H}{\partial q_i},\\
\end{aligned}
$$

:vspace1.5

for $i = 1, \dots, d$, and $d$ being the length of the vectors $\boldsymbol{q}$ and $\boldsymbol{p}$.

## Potential and Kinetic Energy {auto-animate="true"}

. . .

For **Hamiltonian Monte Carlo**, we usually use the following kind of Hamiltonian functions:

$$
H(\boldsymbol{q},\boldsymbol{p}) = \textcolor{var(--secondary-color)}{U(\boldsymbol{q})} + \textcolor{var(--tertiary-color)}{K(\boldsymbol{p})},
$$

. . .

where the [**potential energy** $U(\boldsymbol{q})$]{.col2} is defined as minus the log probability density of the distribution for $\boldsymbol{q}$ that we wish to sample, plus a constant;

. . .

and the [**kinetic energy** $K(\boldsymbol{q})$]{.col3} is given by

$$
\textcolor{var(--tertiary-color)}{K(\boldsymbol{p})} = \boldsymbol{p}'\boldsymbol{M}^{-1}\boldsymbol{p}/2,
$$

where $\boldsymbol{M}$ is a symmetric, p.s.d., and often diagonal mass matrix.

## Hamiltonian Dynamics {auto-animate="true"}

For **Hamiltonian Monte Carlo**, we usually use the following kind of Hamiltonian functions:

$$
H(\boldsymbol{q},\boldsymbol{p}) = \textcolor{var(--secondary-color)}{U(\boldsymbol{q})} + \textcolor{var(--tertiary-color)}{K(\boldsymbol{p})},
$$

Using this specification, the **Hamiltonian functions** can be written as:

$$
\begin{aligned}
\frac{\mathrm{d}q_i}{\mathrm{d}t}&=[\boldsymbol{M}^{-1}\boldsymbol{p}]_i,\\
\frac{\mathrm{d}p_i}{\mathrm{d}t}&=-\frac{\partial U}{\partial q_i}.\\
\end{aligned}
$$



## Properties

. . .

Hamiltonian dynamics fulfill a set of **properties** that make them suitable for use in MCMC updating:

:::{.incremental}
* [**Reversibility.**]{.col1} The mapping from the state at time $t$ to the state at time $t+s$ is one-to-one and hence has an inverse. This means that Markov chain transitions are reversible.
* [**Conservation of the Hamiltonian.**]{.col1} With dynamics as specified, the Hamiltonian $H$ itself is kept invariant. For Metropolis updates, the acceptance probability is one if $H$ is kept invariant (only approximatively achievable).
* [**Volume preservation.**]{.col1} Hamiltonian dynamics preserves volume in $(\boldsymbol{q},\boldsymbol{p})$ space, meaning that we do npt need to account for a change in volume in the acceptance probability for Metropolis updates.
:::

. . .

[**Reversibility**]{.col1} and [**preservation of volume**]{.col1} can be maintained even when the Hamiltonian is approximated.



## Discretization

. . .

For implementation, we need to **discretize** the Hamiltonian equations using a small step size $\varepsilon$: time is then discrete with $t = 0, \varepsilon, 2\varepsilon, 3\varepsilon, \dots$

. . .

The simplest way to approximate the solution is **Euler's method**:

:::{.quitesmall}
$$
p_i(t+\varepsilon) = p_i(t) - \varepsilon \frac{\partial U}{\partial q_i}(q(t)),\qquad q_i(t+\varepsilon) = q_i(t)+\varepsilon\frac{p_i(t)}{m_i}
$$
:::

. . .

We can obtain better results by slightly [**modifying Euler's method**]{.col1}:

:::{.quitesmall}
$$
p_i(t+\varepsilon) = p_i(t) - \varepsilon \frac{\partial U}{\partial q_i}(q(t)),\qquad q_i(t+\varepsilon) = q_i(t)+\varepsilon\frac{\textcolor{var(--primary-color)}{p_i(t+\varepsilon)}}{m_i}
$$
:::

. . .

For even better results, we can use the [**Leapfrog method**]{.col2}:

:::{.quitesmall}
$$
\begin{aligned}
&\textcolor{var(--secondary-color)}{p_i(t+\varepsilon/2)} = p_i(t) - \textcolor{var(--secondary-color)}{(\varepsilon/2)} \frac{\partial U}{\partial q_i}(q(t)),\qquad\qquad q_i(t+\varepsilon) = q_i(t)+\varepsilon\frac{\textcolor{var(--secondary-color)}{p_i(t+\varepsilon/2)}}{m_i},\\
&\textcolor{var(--secondary-color)}{p_i(t+\varepsilon) = p_i(t+\varepsilon/2) - (\varepsilon/2) \frac{\partial U}{\partial q_i}(q(t+e))}
\end{aligned}
$$
:::

## Discretization {.nostretch}

::::{.columns}
:::{.column width="50%" .fragment .centering}
![](figures/discrete.png){width=100%}
:::
:::{.column width="50%" .fragment}

:vspace5 

In this example, $H(q,p)=q^2/2+p^2+2$. The initial state was $q=0,p=1$. We can see that the leapfrog method preserves volume exactly.
:::
::::


## Hamiltonian Monte Carlo

. . .

Using **Hamiltonian dynamics** to sample from a distribution requires translating the density to a [**potential energy**]{.col2} function and introducing [**momentum**]{.col1} variables. We can use the concept of a canonical distribution from statistical mechanics. Given an energy function $E(x)$ for a state $x$, the canonical distribution over states has density $P(x)=(1/Z)\mathrm{exp}(-E(x)/T)$, where $T$ is temperature and $Z$ is a normalizing constant. Using the Hamiltonian $H(\boldsymbol{q},\boldsymbol{p})=U(\boldsymbol{q})+K(\boldsymbol{p})$ as an energy function, we get

$$
P(\boldsymbol{q},\boldsymbol{p}) = \frac{1}{Z}\mathrm{exp}(-U(\boldsymbol{q})/T)\mathrm{exp}(-K(\boldsymbol{p})/T).
$$

. . .

We can see that $\boldsymbol{q}$ and $\boldsymbol{p}$ are independent, and both have canonical distributions. The former will be used to represent our variables of interest, and the latter for momentum.



## The Two Steps of HMC

. . . 

Each **iteration** has **two steps**:

:::{.incremental}
(1) In the [**first step**]{.col1}, new values for the **momentum variables** are drawn from their Gaussian distribution.
(2) In the [**second step**]{.col1}, a **Metropolis** update is performed using Hamiltonian dynamics as explained before. We get a proposed new state $(\boldsymbol{q}^*,\boldsymbol{p}^*)$. This state is accepted as the next state of the Markov chain with probability [$\mathrm{min}\Big(1,\quad \mathrm{exp}(-H(\boldsymbol{q}^*,\boldsymbol{p}^*)+H(\boldsymbol{q},\boldsymbol{p}))\Big)\quad =\quad \mathrm{min}\Big(1,\quad \mathrm{exp} \big(-U(\boldsymbol{q}^*)+U(\boldsymbol{q})-K(\boldsymbol{p}^*)+K(\boldsymbol{p})\big)\Big).$]{.quitesmall}
  If the proposed state is not accepted, the next state is set equal to the current state.
:::

. . .

Only in the very first step of the chain does the **probability density** for $(\boldsymbol{q},\boldsymbol{p})$ change from one step to the next. Also, the algorithm leaves the canonical distribution invariant.

## Benefits of HMC

. . .

We can use HMC to sample only from **continuous** distributions on $\mathbb{R}^d$ for which the **density** function can be evaluated and the **partial derivatives of its log** can be computed.

. . .

It allows us to [**sample more efficiently**]{.col1} from such distributions **than simpler methods** such as random-walk Metropolis.

:::{.centering}
![](figures/benefits.png){width="57%" .fragment}
:::

## Tuning HMC

. . .

However, in order to actually **attain the benefits** of HMC, we have to **properly tune** the [**step size** $\varepsilon$]{.col2} and the [**trajectory length** $L$]{.col4}, i.e., the number of leapfrog steps. [Tuning an [**HMC algorithm**]{.col1} is also more difficult than tuning a **simple Metropolis** method.]{.fragment}


:::{.incremental .li2}
* If the [**step size** $\varepsilon$]{.col2} is **too large**, acceptance rates will be low.
* If the [**step size** $\varepsilon$]{.col2} is **too small**, we will waste computation time.
:::

:::{.incremental .li4}
* If the [**trajectory length** $L$]{.col4} is **too large**, trajectories may be long and still end up back at the starting point.
* If the [**trajectory length** $L$]{.col4} is **too small**, it may not reach far enough into unconstrained directions. In connection with a **small** [**step size** $\varepsilon$]{.col2}, it may lead to slow exploration by a random walk.
:::

## Illustration {.centering}

```{=html}
<iframe data-src="mcmc-demo/app.html"
        width="1280" height="600" data-external="1"></iframe>
```

# The No-U-Turn-Sampler (NUTS)

## Why NUTS?

. . . 

We use **MCMC methods** usually to sample from **complicated distributions**.

:::{.incremental}
* [**Random-walk Metropolis**]{.col4} may take extremely long to converge to a target distribution.
* [**Hamiltonian Monte Carlo**]{.col2} suppresses this random walk behavior.
  * This yields a much lower computational cost. The cost of a sample from a $d$-dimensional target distribution is $O(d^{5/4})$ for HMC, but $O(d^2)$ for random-walk Metropolis.
  * However, HMC requires the researcher to specify $\varepsilon$ and $L$.
  * Especially setting $L$ is costly itself, since it frequently requires multiple costly training runs.
* The [**No-U-Turn-Sampler**]{.col1} ([**NUTS**]{.col1}) eliminates the need to choose $L$, and an extension of it even allows for automatically tuning $\varepsilon$.
:::

## Setting L

:vspace1

:::{.incremental .spacey}
* An $L$ that is **too small** will yield to successive steps being very close to each other. The sampler then exhibits a form of **random walk behavior**.
* An $L$ that is **too large** gives rise to the risk of the trajectory looping back.
* It is **difficult** to find out what the right $L$ in a given case is. What is usually done is to run the sampler preliminarily and check autocorrelation statistics.
:::

## Eliminating the Need to Set L

. . .

The [**No-U-Turn-Sampler**]{.col1} ([**NUTS**]{.col1}) eliminates the need to specify $L$. [The **basic idea** is that it retains the random-walk suppression features of HMC, but additionally evaluates a criterion telling it when it has run for **_long enough_**.]{.fragment}

:::{.incremental}
* The simplest way to think of this is to conceive a metric for whether the distance from the original value $\theta$ to the proposed value $\tilde{\theta}$ has increased:
  $$
  \frac{\mathrm{d}}{\mathrm{d}t}\frac{(\tilde{\theta}-\theta)\cdot(\tilde{\theta}-\theta)}{2}=(\tilde{\theta}-\theta)\cdot\frac{\mathrm{d}}{\mathrm{d}t}(\tilde{\theta}-\theta)=(\tilde{\theta}-\theta)\cdot\tilde{r},
  $$
  where $\tilde{r}$ is the current momentum.
* However, an algorithm that lets the leapfrog steps run until the above expression becomes negative does not guarantee **time reversibility**.
:::

## Eliminating the Need to Set L

:::{.incremental}
* [**NUTS**]{.col1} overcomes the issue by simulating both forward and backward in time, thus building a balanced binary tree via repeated doubling.
* The process is halted when the tree would **double back on itself**, i.e., the proposal would make an **U-turn**.
:::

::::{.columns .fragment}
:::{.column width="60%"}

:vspace1.5

![](figures/doubling.png){width="100%"}
:::
:::{.column width="40%" .quitesmall .col0}
Example of building a binary tree via repeated doubling. Each doubling proceeds by choosing a direction (forwards or backwards in time) uniformly at random, then simulating Hamiltonian dynamics for $2^j$ leapfrog steps in that direction, where $j$ is the number of previous doublings. The figures at top show a trajectory in two dimensions as it evolves over four doublings, and the figures below show the evolution of the binary tree. In this example, the directions chosen were forward (light orange node), backward (yellow nodes), backward (blue nodes), and forward (green nodes).
:::
::::

## Sampling with NUTS
. . .

Roughly speaking, we follow these steps:

:::{.incremental}
1. Sample momentum $r\sim\mathcal N(0,I)$.  
2. Draw the slice variable $u\sim\mathrm{Uniform}([0,\mathrm{exp}(\mathcal{L}(\theta^t)-\frac{1}{2}r\cdot r)$.  
3. Sample $\mathcal{B}$ and $\mathcal{C}$ from their conditional distribution $p(\mathcal{B},\mathcal{C}\mid\theta^t,r,u,\varepsilon)$; where $\mathcal{B}$ collects every state visited by leapfrog integration during an iteration, and $\mathcal{C}\subseteq \mathcal{B}$ are the states eligible as the next state.  
4. Sample $\theta^{t+1},r\sim T(\theta^t,r,\mathcal{C})$, where $T(\theta',r'\mid\theta,r,\mathcal{C})$ is a transition kernel that leaves the uniform distribution over $\mathcal{C}$ invariant.   
:::

## When to Stop Sampling with NUTS

. . .

We then stop expanding the tree when

:::{.incremental}
1. The error in the simulation becomes extremely large, i.e., $\mathcal{L}(\theta)-\frac{1}{2}r\cdot r-\mathrm{log}(u)<-\Delta_{\mathrm{max}}$; or
2. one end of the simulated trajectory makes an U-turn, i.e., continuing with the simulation in any direction would make the left- and rightmost nodes of any subtree, $\theta^-$ and $\theta^+$, move closer together.
:::


## Adaptively Tuning ε

. . .

In addition to letting NUTS choose $L$, we can automatically determine $\varepsilon$.

:::{.incremental}
* Using [**Robbins-Monro**]{.col1}:
  * Use a statistic $H_t$ that describes some aspect of the behavior of an MCMC algorithm at $t\geq 1$, such as the Metropolis acceptance probability.
  * We compare that statistic to a target and adjust the step size by a function of that target, letting adjustments decay with increasing $t$.
* Using [**Dual averaging**]{.col1}:
  * Robbins-Monro gives large weight to early iterations. However, we would want parameters to adapt quickly as the sampler later moves to the stationary regime.
  * Dual averaging techniques decay more slowly and keep a running average of past errors, giving less influence to very early iterations.
:::


## Illustration {.centering}

```{=html}
<iframe data-src="mcmc-demo/app.html"
        width="1280" height="600" data-external="1"></iframe>
```

# Our Implementation

## Two Peak 1D Density

. . .

* Comparison of No-U-Turn Sampler and Metropolis-Hastings for one-dimensional mixture of Gaussians with peaks at -3 & 3:

:::{.centering}
![](figures/two_peak.svg){width="100%" .fragment}
:::


## Two Peak 1D Density

. . .

* However, after 20,000 iterations, even though NUTS obtains a sample with less autocorrelation, the quality of approximation is similar to MH.

:::{.centering}
![](figures/two_peak_2.svg){width="100%" .fragment}
:::

## Three Peak 2D Density 

:::{.incremental}
* Real strength of NUTS shows for multidimensional distributions.
* Already after 100 iterations, NUTS has visited all three peaks.
:::

:::{.centering}
![](figures/three_peak_100_1000.svg){width="100%" .fragment}
:::



## Three Peak 2D Density 

* NUTS particularly good at exploring concentrations that are more distant to other peaks

:::{.centering}
![](figures/Rplot02.svg){width="100%" .fragment}
:::

## Three Peak 2D Density 

:::{.incremental}
* Step size adpatively set in Dual-Averaging procedure.
* Standard setting: fix epsilon after first half of iterations.
:::

:::{.centering}
![](figures/three_peak_eps.svg){width="100%" .fragment}
:::

## Three Peak 2D Density

:::{.centering}
![](figures/three_peak_acf.svg){width="80%" .fragment}
:::

## References 

::: {#refs}
:::

# Appendix 

## NUTS algorithm in R

:vspace1.5


```{.r}
# No-U-Turn sampler
leapfrog <- function(theta, r, grad_log_prob, eps) {
  r_half   <- r + 0.5 * eps * grad_log_prob(theta)
  theta_new<- theta + eps * r_half
  r_new    <- r_half + 0.5 * eps * grad_log_prob(theta_new)
  list(theta = theta_new, r = r_new)
}

find_reasonable_epsilon <- function(theta, grad_log_prob, log_prob) {
  eps <- 1
  r   <- rnorm(length(theta))
  lp0 <- log_prob(theta) - 0.5 * sum(r^2)
  lf  <- leapfrog(theta, r, grad_log_prob, eps)
  lp1 <- log_prob(lf$theta) - 0.5 * sum(lf$r^2)
  a   <- exp(lp1 - lp0)
  dir <- if (a > 0.5) 1 else -1
  while (a^dir > 2^(-dir)) {
    eps <- eps * 2^dir
    lf  <- leapfrog(theta, r, grad_log_prob, eps)
    lp1 <- log_prob(lf$theta) - 0.5 * sum(lf$r^2)
    a   <- exp(lp1 - lp0)
  }
  eps
}

build_tree <- function(theta, r, u, v, j, eps, log_prob, grad_log_prob, delta_max = 1000) {
  if (j == 0) {
    lf <- leapfrog(theta, r, grad_log_prob, v * eps)
    lp <- log_prob(lf$theta) - 0.5 * sum(lf$r^2)
    n  <- as.integer(u <= exp(lp))
    s  <- as.integer(lp - log(u) > -delta_max)
    alpha <- min(1, exp(lp - (log_prob(theta) - 0.5*sum(r^2))))
    list(theta_minus = lf$theta, r_minus = lf$r,
         theta_plus  = lf$theta, r_plus  = lf$r,
         theta_prop  = lf$theta, n_prop  = n,
         s_prop      = s,     alpha    = alpha,
         n_alpha     = 1)
  } else {
    bt1 <- build_tree(theta, r, u, v, j-1, eps, log_prob, grad_log_prob, delta_max)
    if (bt1$s_prop == 1) {
      if (v == -1) {
        bt2 <- build_tree(bt1$theta_minus, bt1$r_minus, u, v, j-1, eps, log_prob, grad_log_prob, delta_max)
        theta_minus <- bt2$theta_minus; r_minus <- bt2$r_minus
        theta_plus  <- bt1$theta_plus;  r_plus  <- bt1$r_plus
      } else {
        bt2 <- build_tree(bt1$theta_plus, bt1$r_plus, u, v, j-1, eps, log_prob, grad_log_prob, delta_max)
        theta_minus <- bt1$theta_minus; r_minus <- bt1$r_minus
        theta_plus  <- bt2$theta_plus;  r_plus  <- bt2$r_plus
      }
      den  <- bt1$n_prop + bt2$n_prop
      if (den > 0) {
        if (runif(1) < bt2$n_prop / den) {
          theta_prop <- bt2$theta_prop
        } else {
          theta_prop <- bt1$theta_prop
        }
      } else {
        theta_prop <- bt1$theta_prop
      }
      r_sum <- bt2$r_plus + bt2$r_minus
      s_prop <- as.integer(bt1$s_prop == 1 && bt2$s_prop == 1 &&
                             sum(r_sum * (bt2$theta_plus - bt2$theta_minus)) >= 0)
      list(theta_minus = theta_minus, r_minus = r_minus,
           theta_plus  = theta_plus,  r_plus  = r_plus,
           theta_prop  = theta_prop,
           n_prop      = bt1$n_prop + bt2$n_prop,
           s_prop      = s_prop,
           alpha       = bt1$alpha + bt2$alpha,
           n_alpha     = bt1$n_alpha + bt2$n_alpha)
    } else {
      bt1
    }
  }
}

nuts <- function(log_prob, grad_log_prob, theta0, n_iter, adapt_steps = floor(n_iter/2),
                 delta = 0.65, max_depth = 10) {
  d <- length(theta0)
  samples <- matrix(NA, n_iter, d)
  eps_hist <- numeric(n_iter)
  theta <- theta0
  eps   <- find_reasonable_epsilon(theta, grad_log_prob, log_prob)
  mu    <- log(10 * eps)
  eps_bar <- 1
  H_bar   <- 0
  gamma <- 0.05; t0 <- 10; kappa <- 0.75
  
  for (i in 1:n_iter) {
    eps_hist[i] <- eps
    r0 <- rnorm(d)
    joint0 <- log_prob(theta) - 0.5 * sum(r0^2)
    u <- exp(joint0) * runif(1)
    theta_minus <- theta; theta_plus <- theta
    r_minus <- r0;       r_plus   <- r0
    theta_prop <- theta
    j <- 0; n_prop <- 1; s_prop <- 1; alpha_sum <- 0; n_alpha_sum <- 0
    
    while (s_prop == 1 && j < max_depth) {
      v <- sample(c(-1,1),1)
      if (v == -1) {
        bt <- build_tree(theta_minus, r_minus, u, v, j, eps, log_prob, grad_log_prob)
        theta_minus <- bt$theta_minus; r_minus <- bt$r_minus
      } else {
        bt <- build_tree(theta_plus, r_plus, u, v, j, eps, log_prob, grad_log_prob)
        theta_plus  <- bt$theta_plus;  r_plus  <- bt$r_plus
      }
      if (bt$s_prop == 1 && runif(1) < bt$n_prop / n_prop) {
        theta_prop <- bt$theta_prop
      }
      n_prop    <- n_prop + bt$n_prop
      s_prop    <- bt$s_prop * as.integer(sum((theta_plus - theta_minus)*r_minus)>=0) *
        as.integer(sum((theta_plus - theta_minus)*r_plus )>=0)
      alpha_sum   <- alpha_sum + bt$alpha
      n_alpha_sum <- n_alpha_sum + bt$n_alpha
      j <- j + 1
    }
    
    acc_rate <- alpha_sum / n_alpha_sum
    if (i <= adapt_steps) {
      H_bar <- (1 - 1/(i + t0)) * H_bar + (delta - acc_rate)/(i + t0)
      log_eps <- mu - sqrt(i)/gamma * H_bar
      eta <- i^(-kappa)
      eps_bar <- exp((1 - eta) * log(eps_bar) + eta * log_eps)
      eps <- exp(log_eps)
    } else {
      eps <- eps_bar
    }
    
    theta <- theta_prop
    samples[i, ] <- theta
    cat(sprintf("Iter %d: eps=%.5f, acc=%.3f\n", i, eps, acc_rate))
    
  }
  samples
}

```








