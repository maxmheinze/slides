---
title: "Hamiltonian Monte Carlo and the No-U-Turn-Sampler"
subtitle: "Computational Statistics Project"
author: 
- "Jonathan Fitter ([jfitter@wu.ac.at](mailto:jonathan.fitter@wu.ac.at))"
- "Max Heinze ([mheinze@wu.ac.at](mailto:mheinze@wu.ac.at))"
institute:
- "Department of Economics, Vienna University of Economics and Business (WU Vienna)"
date: "2025-05-27"
date-format: long
lang: en
format: 
  revealjs:
    theme: [default, ../style/mhslides_flippedcols.css]
    width: 1280
    height: 720
    margin: 0
    progress: false
    overview: false
    highlight-style: github
    slideNumber: true
    html-math-method: mathjax
    embed-resources: false
    pdfMaxPagesPerSlide: 1
    pdfSeparateFragments: false
    template-partials:
      - ../style/title-slide.html
    filters:
      - ../style/section-header.lua
      - _filters/pdf-to-svg.lua
      - _filters/space.lua
      - quarto-kroki
      - _filters/svgfonts.lua
engine: knitr
bibliography: references.bib
csl: ../style/apa.csl
nocite: |
  @hoffman2014
---

# Intro and Recap

## Markov Chain

. . . 

A [**Markov chain**]{.col1} is a stochastic process $\{X_t\}$ indexed by time $t\geq 0$.

:::{.incremental}
* We call the **set of all values** that $\{X_t\}$ can assume the [**state space**]{.col1} of the Markov Chain.
* $\{X-t\}$ is called a Markov Chain if it fulfills the [**Markov property**]{.col1} that
  $$
  P(X_{t+1}=j\mid X_0=i_0,\dots,X_t=i_t)=P(X_{t+1}=j\mid X_t=i_t),
  $$
  i.e., that the conditional distribution of the next state depends only on the current state.
:::

. . .

[**Markov Chain Monte Carlo (MCMC)**]{.col1} methods make use of Markov chains to sample from a **target distribution**. 

## Markov Chain

::::{.columns}
:::{.column width="40%" .fragment}
::: {.w100 .centering}
```{kroki-tikz}
\documentclass{standalone}
\usepackage{tikz}
\usepackage{amsmath}
\usetikzlibrary{automata,positioning,arrows.meta}

\begin{document}
\begin{tikzpicture}[->,>=Stealth,shorten >=1pt,auto,node distance=3cm,semithick]
  \node[state] (A) {$A$};
  \node[state] (B) [right of=A] {$B$};
  \node[state] (C) [below of=A] {$C$};

  \path
    (A) edge [loop above] node {0.5} ()
        edge              node {0.3} (B)
        edge [bend left]  node {0.2} (C)
    (B) edge [loop above] node {0.6} ()
        edge [bend left]  node {0.4} (C)
    (C) edge [loop below] node {0.7} ()
        edge [bend left]  node {0.3} (A);
\end{tikzpicture}
\end{document}
```
:::
:::
:::{.column width="60%" .fragment}
Markov chains are

:::{.incremental}
* **irreducible**, i.e. every state can eventually be reached from any state;
* **aperiodic**, i.e. reverting to a state is not only possible after a multiple of a certain number of steps;
* **positive recurrent**, i.e. for all states, the expected number of transitions to return to that state is finite.
:::

[For such Markov Chains, [**transition probabilities**]{.col1} converge to a unique stationary distribution on the state space.]{.fragment}

:::
::::

## Metropolis-Hastings

::::{.columns}
:::{.column width="40%" .fragment}

:vspace2

::: {.w100 .centering}
```{kroki-tikz}
\documentclass{standalone}
\usepackage{tikz}
\usetikzlibrary{automata,arrows,positioning}
\begin{document}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,node distance=3cm]
  \node[state] (i) {$i$};
  \node[state] (j) [right=of i] {$j$};
  \node[state] (k) [below=of i] {$k$};
  \path
    (i) edge[bend left] node[above] {$g(j\mid i)\,\alpha(i,j)$} (j)
        edge[loop above]  node           {$1-\sum_{h\neq i}g(h\mid i)\,\alpha(i,h)$} ()
    (j) edge[bend left] node[below] {$g(i\mid j)\,\alpha(j,i)$} (i)
        edge[bend left] node[above] {$g(k\mid j)\,\alpha(j,k)$} (k)
    (k) edge[bend left] node[below] {$g(i\mid k)\,\alpha(k,i)$} (i)
        edge[loop below]  node           {$1-\sum_{h\neq k}g(h\mid k)\,\alpha(k,h)$} ();
\end{tikzpicture}
\end{document}
```
:::
:::
:::{.column width="60%" .fragment}

**Metropolis-Hastings** is a class of MCMC methods. 

:::{.incremental}
*	Propose $Y\sim g(\cdot\mid X_t)$, compute $\alpha(i,j)=\min\Bigl(1,\frac{f(j)\,g(i\mid j)}{f(i)\,g(j\mid i)}\Bigr)$.
* If $U\le\alpha(i,j)$, move $i\to j$; else stay at $i$.
* Chain is reversible and has $f$ as its stationary distribution.
:::



:::
::::




# Hamiltonian Monte Carlo

## Origins in Physics

::::{.columns}
:::{.column width="45%" .fragment .centering}

:vspace3

![](figures/puck.svg){width=100%}
:::
:::{.column width="55%" .fragment}

Imagine a **frictionless puck** on a **non-even surface**. The [**state of this system**]{.col1} is given by

:::{.incremental}
*	$\boldsymbol{q}$, the [**position**]{.col1} of the puck;
* $\boldsymbol{p}$, the [**momentum**]{.col4} of the puck, given by $\mathrm{mass}\times\mathrm{velocity}$.
* If the puck encounters a **rising slope**, momentum will decrease; if it encounters a **falling slope**, momentum will increase.
* The higher the surface at a position, the higher the puck's **potential energy**, and the lower its **kinetic energy**.
:::


:::
::::

<!---

## Origins in Physics

:::{.incremental}
* We simulate the path of the puck starting from a position in this Hamiltonian system with a given kinetic energy
* Pre-determined step-size and number of steps (use of discretizer)
* After stopping at the final position along the path, we randomly introduce a new level of kinetic energy
* we simulate the path,...
:::

-->


## From Physics to Statistics

. . .

:vspace1

In **non-physical** applications, 

:vspace1.5

:::{.incremental .spacey}
* the [**position**]{.col1} $\boldsymbol{q}$ corresponds to the [**variables of interest**]{.col1},
* the [**potential energy**]{.col2} $\boldsymbol{p}$ corresponds to the [**negative log of the probability density**]{.col2} for these variables, and
* the [**momentum**]{.col4} is an auxiliary variable that facilitates exploration of the target distribution space.
:::


## Hamiltonian Dynamics

. . .

The system is described by a so-called [**Hamiltonian equation**]{.col1}, $H(q_p)$. Its partial derivatives, the **equations of motion**, determine how $\boldsymbol{q}$ and $\boldsymbol{p}$ change over time:

. . .

:vspace1.5

$$
\begin{aligned}
\frac{\mathrm{d}q_i}{\mathrm{d}t}&=\frac{\partial H}{\partial p_i},\\
\frac{\mathrm{d}p_i}{\mathrm{d}t}&=-\frac{\partial H}{\partial q_i},\\
\end{aligned}
$$

:vspace1.5

for $i = 1, \dots, d$, and $d$ being the length of the vectors $\boldsymbol{q}$ and $\boldsymbol{p}$.

## Potential and Kinetic Energy {auto-animate="true"}

. . .

For **Hamiltonian Monte Carlo**, we usually use the following kind of Hamiltonian functions:

$$
H(\boldsymbol{q},\boldsymbol{p}) = \textcolor{var(--secondary-color)}{U(\boldsymbol{q})} + \textcolor{var(--tertiary-color)}{K(\boldsymbol{p})},
$$

. . .

where the [**potential energy** $U(\boldsymbol{q})$]{.col2} is defined as minus the log probability density of the distribution for $\boldsymbol{q}$ that we wish to sample, plus a constant;

. . .

and the [**kinetic energy** $K(\boldsymbol{q})$]{.col3} is given by

$$
\textcolor{var(--tertiary-color)}{K(\boldsymbol{p})} = \boldsymbol{p}'\boldsymbol{M}^{-1}\boldsymbol{p}/2,
$$

where $\boldsymbol{M}$ is a symmetric, p.s.d., and often diagonal mass matrix.

## Hamiltonian Dynamics {auto-animate="true"}

For **Hamiltonian Monte Carlo**, we usually use the following kind of Hamiltonian functions:

$$
H(\boldsymbol{q},\boldsymbol{p}) = \textcolor{var(--secondary-color)}{U(\boldsymbol{q})} + \textcolor{var(--tertiary-color)}{K(\boldsymbol{p})},
$$

Using this specification, the **Hamiltonian functions** can be written as:

$$
\begin{aligned}
\frac{\mathrm{d}q_i}{\mathrm{d}t}&=[\boldsymbol{M}^{-1}\boldsymbol{p}]_i,\\
\frac{\mathrm{d}p_i}{\mathrm{d}t}&=-\frac{\partial U}{\partial q_i}.\\
\end{aligned}
$$



## Properties

. . .

Hamiltonian dynamics fulfill a set of **properties** that make them suitable for use in MCMC updating:

:::{.incremental}
* [**Reversibility.**]{.col1} The mapping from the state at time $t$ to the state at time $t+s$ is one-to-one and hence has an inverse. This means that Markov chain transitions are reversible.
* [**Conservation of the Hamiltonian.**]{.col1} With dynamics as specified, the Hamiltonian $H$ itself is kept invariant. For Metropolis updates, the acceptance probability is one if $H$ is kept invariant (only approximatively achievable).
* [**Volume preservation.**]{.col1} Hamiltonian dynamics preserves volume in $(\boldsymbol{q},\boldsymbol{p})$ space, meaning that we do npt need to account for a change in volume in the acceptance probability for Metropolis updates.
:::

. . .

[**Reversibility**]{.col1} and [**preservation of volume**]{.col1} can be maintained even when the Hamiltonian is approximated.



## Discretization

. . .

For implementation, we need to **discretize** the Hamiltonian equations using a small step size $\varepsilon$: time is then discrete with $t = 0, \varepsilon, 2\varepsilon, 3\varepsilon, \dots$

. . .

The simplest way to approximate the solution is **Euler's method**:

:::{.quitesmall}
$$
p_i(t+\varepsilon) = p_i(t) - \varepsilon \frac{\partial U}{\partial q_i}(q(t)),\qquad q_i(t+\varepsilon) = q_i(t)+\varepsilon\frac{p_i(t)}{m_i}
$$
:::

. . .

We can obtain better results by slightly [**modifying Euler's method**]{.col1}:

:::{.quitesmall}
$$
p_i(t+\varepsilon) = p_i(t) - \varepsilon \frac{\partial U}{\partial q_i}(q(t)),\qquad q_i(t+\varepsilon) = q_i(t)+\varepsilon\frac{\textcolor{var(--primary-color)}{p_i(t+\varepsilon)}}{m_i}
$$
:::

. . .

For even better results, we can use the [**Leapfrog method**]{.col2}:

:::{.quitesmall}
$$
\begin{aligned}
&\textcolor{var(--secondary-color)}{p_i(t+\varepsilon/2)} = p_i(t) - \textcolor{var(--secondary-color)}{(\varepsilon/2)} \frac{\partial U}{\partial q_i}(q(t)),\qquad\qquad q_i(t+\varepsilon) = q_i(t)+\varepsilon\frac{\textcolor{var(--secondary-color)}{p_i(t+\varepsilon/2)}}{m_i},\\
&\textcolor{var(--secondary-color)}{p_i(t+\varepsilon) = p_i(t+\varepsilon/2) - (\varepsilon/2) \frac{\partial U}{\partial q_i}(q(t+e))}
\end{aligned}
$$
:::

## Discretization {.nostretch}

::::{.columns}
:::{.column width="50%" .fragment .centering}
![](figures/discrete.png){width=100%}
:::
:::{.column width="50%" .fragment}

:vspace5 

In this example, $H(q,p)=q^2/2+p^2+2$. The initial state was $q=0,p=1$. We can see that the leapfrog method preserves volume exactly.
:::
::::


## Hamiltonian Monte Carlo

. . .

Using **Hamiltonian dynamics** to sample from a distribution requires translating the density to a [**potential energy**]{.col2} function and introducing [**momentum**]{.col1} variables. We can use the concept of a canonical distribution from statistical mechanics. Given an energy function $E(x)$ for a state $x$, the canonical distribution over states has density $P(x)=(1/Z)\mathrm{exp}(-E(x)/T)$, where $T$ is temperature and $Z$ is a normalizing constant. Using the Hamiltonian $H(\boldsymbol{q},\boldsymbol{p})=U(\boldsymbol{q})+K(\boldsymbol{p})$ as an energy function, we get

$$
P(\boldsymbol{q},\boldsymbol{p}) = \frac{1}{Z}\mathrm{exp}(-U(\boldsymbol{q})/T)\mathrm{exp}(-K(\boldsymbol{p})/T).
$$

. . .

We can see that $\boldsymbol{q}$ and $\boldsymbol{p}$ are independent, and both have canonical distributions. The former will be used to represent our variables of interest, and the latter for momentum.



## The Two Steps of HMC

. . . 

Each **iteration** has **two steps**:

:::{.incremental}
(1) In the [**first step**]{.col1}, new values for the **momentum variables** are drawn from their Gaussian distribution.
(2) In the [**second step**]{.col1}, a **Metropolis** update is performed using Hamiltonian dynamics as explained before. We get a proposed new state $(\boldsymbol{q}^*,\boldsymbol{p}^*)$. This state is accepted as the next state of the Markov chain with probability [$\mathrm{min}\Big(1,\quad \mathrm{exp}(-H(\boldsymbol{q}^*,\boldsymbol{p}^*)+H(\boldsymbol{q},\boldsymbol{p}))\Big)\quad =\quad \mathrm{min}\Big(1,\quad \mathrm{exp} \big(-U(\boldsymbol{q}^*)+U(\boldsymbol{q})-K(\boldsymbol{p}^*)+K(\boldsymbol{p})\big)\Big).$]{.quitesmall}
  If the proposed state is not accepted, the next state is set equal to the current state.
:::

. . .

Only in the very first step of the chain does the **probability density** for $(\boldsymbol{q},\boldsymbol{p})$ change from one step to the next. Also, the algorithm leaves the canonical distribution invariant.

## Benefits of HMC

. . .

We can use HMC to sample only from **continuous** distributions on $\mathbb{R}^d$ for which the **density** function can be evaluated and the **partial derivatives of its log** can be computed.

. . .

It allows us to [**sample more efficiently**]{.col1} from such distributions **than simpler methods** such as random-walk Metropolis.

:::{.centering}
![](figures/benefits.png){width="57%" .fragment}
:::

## Tuning HMC

. . .

However, in order to actually **attain the benefits** of HMC, we have to **properly tune** the [**step size** $\varepsilon$]{.col2} and the [**trajectory length** $L$]{.col4}, i.e., the number of leapfrog steps. [Tuning an [**HMC algorithm**]{.col1} is also more difficult than tuning a **simple Metropolis** method.]{.fragment}


:::{.incremental .li2}
* If the [**step size** $\varepsilon$]{.col2} is **too large**, acceptance rates will be low.
* If the [**step size** $\varepsilon$]{.col2} is **too small**, we will waste computation time.
:::

:::{.incremental .li4}
* If the [**trajectory length** $L$]{.col4} is **too large**, trajectories may be long and still end up back at the starting point.
* If the [**trajectory length** $L$]{.col4} is **too small**, it may not reach far enough into unconstrained directions. In connection with a **small** [**step size** $\varepsilon$]{.col2}, it may lead to slow exploration by a random walk.
:::

## Illustration {.centering}

```{=html}
<iframe src="mcmc-demo/app.html"
        width="1280" height="600"></iframe>
```

# The No-U-Turn-Sampler (NUTS)

## Why NUTS?

## Setting L

## Eliminating the Need to Set L

## NUTS

## Adaptively Tuning Îµ

## Dual-Averaging NUTS

## Illustration {.centering}

```{=html}
<iframe src="mcmc-demo/app.html"
        width="1280" height="600"></iframe>
```

# Our Implementation

## Slide

## References 

:::{.centering .titlebox1}
_This list is **scrollable**._
:::

::: {#refs}
:::






